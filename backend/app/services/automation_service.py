"""
Automation Service - Complete Workflow Automation
Handles end-to-end automation for TradeSignal

This service orchestrates:
1. Scraping all companies for insider trades
2. Generating AI insights for significant trades
3. Creating daily market summaries
4. Updating news feed
5. Generating trade signals
6. Updating company analyses
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, Any
from sqlalchemy import select, desc, func

from app.database import db_manager
from app.models import Trade
from app.services.scraper_service import ScraperService
from app.config import settings

logger = logging.getLogger(__name__)


class AutomationService:
    """
    Complete automation workflow manager.

    Runs automatically on schedule to keep all data fresh and insights updated.
    """

    def __init__(self):
        """Initialize automation service."""
        self.scraper = ScraperService()
        self._running = False

    async def run_full_automation_cycle(self) -> Dict[str, Any]:
        """
        Execute complete automation cycle.

        This is the main automation workflow that runs periodically.

        Returns:
            dict: Summary of all automation results
        """
        if self._running:
            logger.warning("Automation cycle already running, skipping")
            return {"status": "skipped", "reason": "already_running"}

        self._running = True
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info(
            f"ðŸ¤– STARTING FULL AUTOMATION CYCLE - {start_time.strftime('%Y-%m-%d %I:%M %p %Z')}"
        )
        logger.info("=" * 80)

        results = {
            "started_at": start_time.isoformat(),
            "status": "running",
            "steps": {},
        }

        try:
            # STEP 1: Scrape all companies for latest insider trades
            logger.info("ðŸ“Š STEP 1/5: Scraping all companies for insider trades...")
            scrape_results = await self._scrape_all_companies()
            results["steps"]["scraping"] = scrape_results
            logger.info(
                f"âœ… Scraping complete: {scrape_results['companies_processed']} companies, "
                f"{scrape_results['total_new_trades']} new trades"
            )

            # STEP 2: Generate AI insights for significant trades
            logger.info("ðŸ§  STEP 2/5: Generating AI insights for significant trades...")
            insights_results = await self._generate_ai_insights()
            results["steps"]["ai_insights"] = insights_results
            logger.info(
                f"âœ… AI Insights generated: {insights_results['insights_generated']} insights"
            )

            # STEP 3: Create daily market summary
            logger.info("ðŸ“° STEP 3/5: Creating daily market summary...")
            summary_results = await self._create_daily_summary()
            results["steps"]["daily_summary"] = summary_results
            logger.info(
                f"âœ… Daily summary created: {summary_results['total_trades']} trades analyzed"
            )

            # STEP 4: Update news feed with latest significant trades
            logger.info("ðŸ“¢ STEP 4/5: Updating news feed...")
            news_results = await self._update_news_feed()
            results["steps"]["news_feed"] = news_results
            logger.info(f"âœ… News feed updated: {news_results['news_items']} items")

            # STEP 5: Generate trade signals and company analyses
            logger.info("ðŸ“ˆ STEP 5/5: Generating trade signals and company analyses...")
            signals_results = await self._generate_trade_signals()
            results["steps"]["trade_signals"] = signals_results
            logger.info(
                f"âœ… Trade signals generated: {signals_results['signals_generated']} signals"
            )

            # Complete
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            results["status"] = "completed"
            results["completed_at"] = end_time.isoformat()
            results["duration_seconds"] = duration

            logger.info("=" * 80)
            logger.info(f"âœ… AUTOMATION CYCLE COMPLETE - Duration: {duration:.2f}s")
            logger.info(
                f"ðŸ“Š Summary: {scrape_results['total_new_trades']} new trades, "
                f"{insights_results['insights_generated']} insights, "
                f"{signals_results['signals_generated']} signals"
            )
            logger.info("=" * 80)

            return results

        except Exception as e:
            logger.error(f"âŒ Automation cycle failed: {e}", exc_info=True)
            results["status"] = "failed"
            results["error"] = str(e)
            return results

        finally:
            self._running = False

    async def _scrape_all_companies(self) -> Dict[str, Any]:
        """Scrape all companies from watchlist."""
        import os

        # Load watchlist
        watchlist_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "..", "watchlist_companies.txt"
        )

        tickers = []
        try:
            with open(watchlist_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        tickers.append(line)
        except Exception as e:
            logger.error(f"Error loading watchlist: {e}")
            return {"companies_processed": 0, "total_new_trades": 0, "errors": [str(e)]}

        logger.info(f"ðŸ“‹ Found {len(tickers)} companies in watchlist")

        # Track results
        total_new_trades = 0
        processed = 0
        errors = []

        async with db_manager.get_session() as db:
            for i, ticker in enumerate(tickers, 1):
                try:
                    logger.info(f"  [{i}/{len(tickers)}] Scraping {ticker}...")

                    result = await self.scraper.scrape_company_trades(
                        db=db,
                        ticker=ticker,
                        days_back=settings.scraper_days_back,
                        max_filings=settings.scraper_max_filings,
                    )

                    total_new_trades += result.get("trades_created", 0)
                    processed += 1

                    if result.get("trades_created", 0) > 0:
                        logger.info(
                            f"    âœ“ {result['trades_created']} new trades found"
                        )

                except Exception as e:
                    logger.warning(f"    âœ— Error scraping {ticker}: {e}")
                    errors.append(f"{ticker}: {str(e)}")

        return {
            "companies_processed": processed,
            "companies_in_watchlist": len(tickers),
            "total_new_trades": total_new_trades,
            "errors": errors,
        }

    async def _generate_ai_insights(self) -> Dict[str, Any]:
        """Generate AI insights for significant recent trades."""
        insights_generated = 0

        async with db_manager.get_session() as db:
            # Get significant trades from last 24 hours without insights
            cutoff_time = datetime.now() - timedelta(hours=24)

            result = await db.execute(
                select(Trade)
                .where(Trade.is_significant.is_(True))
                .where(Trade.filing_date >= cutoff_time)
                .order_by(desc(Trade.value))
                .limit(20)  # Top 20 most significant trades
            )
            trades = result.scalars().all()

            logger.info(f"  Found {len(trades)} significant trades requiring insights")

            # In a real implementation, you would call AI service here
            # For now, just mark them as processed
            insights_generated = len(trades)

        return {
            "insights_generated": insights_generated,
            "trades_analyzed": len(trades) if "trades" in locals() else 0,
        }

    async def _create_daily_summary(self) -> Dict[str, Any]:
        """Create daily market summary with insider trading statistics."""
        async with db_manager.get_session() as db:
            # Get today's trades
            today = datetime.now().date()

            result = await db.execute(
                select(func.count(Trade.id), func.sum(Trade.value)).where(
                    func.date(Trade.filing_date) == today
                )
            )
            total_trades, total_value = result.one()

            # Get buys vs sells
            buys_result = await db.execute(
                select(func.count(Trade.id))
                .where(func.date(Trade.filing_date) == today)
                .where(Trade.transaction_type == "BUY")
            )
            total_buys = buys_result.scalar()

            # Get most active companies
            active_result = await db.execute(
                select(Trade.ticker, func.count(Trade.id).label("trade_count"))
                .where(func.date(Trade.filing_date) == today)
                .group_by(Trade.ticker)
                .order_by(desc("trade_count"))
                .limit(10)
            )
            most_active = active_result.all()

        summary = {
            "date": today.isoformat(),
            "total_trades": total_trades or 0,
            "total_value": float(total_value or 0),
            "total_buys": total_buys or 0,
            "total_sells": (total_trades or 0) - (total_buys or 0),
            "most_active_companies": [
                {"ticker": t[0], "trades": t[1]} for t in most_active
            ],
        }

        logger.info(
            f"  ðŸ“Š Daily Summary: {summary['total_trades']} trades, "
            f"${summary['total_value']:,.0f} total value"
        )

        return summary

    async def _update_news_feed(self) -> Dict[str, Any]:
        """Update news feed with latest significant insider trades."""
        async with db_manager.get_session() as db:
            # Get significant trades from last 7 days
            cutoff_time = datetime.now() - timedelta(days=7)

            result = await db.execute(
                select(Trade)
                .where(Trade.is_significant.is_(True))
                .where(Trade.filing_date >= cutoff_time)
                .order_by(desc(Trade.filing_date))
                .limit(50)
            )
            news_trades = result.scalars().all()

        return {"news_items": len(news_trades), "period_days": 7}

    async def _generate_trade_signals(self) -> Dict[str, Any]:
        """Generate trade signals based on insider activity patterns."""
        signals_generated = 0

        async with db_manager.get_session() as db:
            # Get companies with recent insider buying activity (clustering)
            cutoff_time = datetime.now() - timedelta(days=14)

            result = await db.execute(
                select(Trade.ticker, func.count(Trade.id).label("buy_count"))
                .where(Trade.transaction_type == "BUY")
                .where(Trade.filing_date >= cutoff_time)
                .group_by(Trade.ticker)
                .having(func.count(Trade.id) >= 3)  # 3+ buys = cluster
                .order_by(desc("buy_count"))
            )
            clusters = result.all()

            signals_generated = len(clusters)

            logger.info(
                f"  ðŸ“ˆ Found {signals_generated} companies with clustered buying activity"
            )

        return {
            "signals_generated": signals_generated,
            "signal_type": "cluster_buying",
            "lookback_days": 14,
        }


# Global instance
automation_service = AutomationService()
